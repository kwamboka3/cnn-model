{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Supervised Learning Classification Models\n",
    "\n",
    "We have already discussed that sentiment analysis, also popularly known as opinion analysis or opinion mining is one of the most important applications of NLP. The key idea is to predict the potential sentiment of a body of text based on the textual content. In this sub-unit, we will be exploring supervised learning models. \n",
    "\n",
    "![](sentiment_cover.png)\n",
    "\n",
    "Another way to build a model to understand the text content and predict the sentiment of the text based reviews is to use supervised machine learning. To be more specific, we will be using classification models for solving this problem. We will be building an automated sentiment text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n",
    "\n",
    "1.\tPrepare train and test datasets (optionally a validation dataset)\n",
    "2.\tPre-process and normalize text documents\n",
    "3.\tFeature Engineering \n",
    "4.\tModel training\n",
    "5.\tModel prediction and evaluation\n",
    "\n",
    "These are the major steps for building our system. Optionally the last step would be to deploy the model in your server or on the cloud. The following figure shows a detailed workflow for building a standard text classification system with supervised learning (classification) models.\n",
    "\n",
    "![](sentiment_classifier_workflow.png)\n",
    "\n",
    "\n",
    "In our scenario, documents indicate the movie reviews and classes indicate the review sentiments which can either be positive or negative making it a binary classification problem. We will build models using both traditional machine learning methods and newer deep learning in the subsequent sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(r'../Unit 11 - Sentiment Analysis - Unsupervised Learning/movie_reviews.csv.bz2', compression='bz2')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "reviews = dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values\n",
    "\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Wrangling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('but')\n",
    "stop_words.remove('not')\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def simple_stemming(text, stemmer=ps):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def pre_process_document(document):\n",
    "    \n",
    "    # strip HTML\n",
    "    document = strip_html_tags(document)\n",
    "    \n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    \n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    \n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    \n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)\n",
    "               \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, remove_digits=True)  \n",
    "\n",
    "    # stemming text\n",
    "    document = simple_stemming(document)      \n",
    "    \n",
    "    # remove stopwords\n",
    "    document = remove_stopwords(document, is_lower_case=True, stopwords=stop_words)\n",
    "        \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (35000, 138566)  Test features shape: (15000, 138566)\n",
      "TFIDF model:> Train features shape: (35000, 138566)  Test features shape: (15000, 138566)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
    "\n",
    "# train model\n",
    "lr.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "lr_bow_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9041\n",
      "Precision: 0.9041\n",
      "Recall: 0.9041\n",
      "F1 Score: 0.9041\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.90      0.90      0.90      7510\n",
      "    negative       0.90      0.90      0.90      7490\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6793      717\n",
      "        negative        721     6769\n"
     ]
    }
   ],
   "source": [
    "import model_evaluation_utils as meu\n",
    "\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on TF-IDF features\n",
    "\n",
    "# train model\n",
    "lr.fit(tv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "lr_tfidf_predictions = lr.predict(tv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9036\n",
      "Precision: 0.9037\n",
      "Recall: 0.9036\n",
      "F1 Score: 0.9036\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.90      0.91      0.90      7510\n",
      "    negative       0.91      0.90      0.90      7490\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6834      676\n",
      "        negative        770     6720\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest model on BOW features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate model\n",
    "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "\n",
    "# train model\n",
    "rf.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "rf_bow_predictions = rf.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8734\n",
      "Precision: 0.8735\n",
      "Recall: 0.8734\n",
      "F1 Score: 0.8734\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.87      0.88      0.87      7510\n",
      "    negative       0.88      0.87      0.87      7490\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     15000\n",
      "   macro avg       0.87      0.87      0.87     15000\n",
      "weighted avg       0.87      0.87      0.87     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6618      892\n",
      "        negative       1007     6483\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=rf_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest model on TF-IDF features\n",
    "\n",
    "# train model\n",
    "rf.fit(tv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "rf_tfidf_predictions = rf.predict(tv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8761\n",
      "Precision: 0.8761\n",
      "Recall: 0.8761\n",
      "F1 Score: 0.8761\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.87      0.88      0.88      7510\n",
      "    negative       0.88      0.87      0.88      7490\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6599      911\n",
      "        negative        947     6543\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=rf_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newer Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction class label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "num_classes=2 \n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                       for text in norm_train_reviews]\n",
    "y_tr = le.fit_transform(train_sentiments)\n",
    "y_train = keras.utils.to_categorical(y_tr, num_classes)\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                       for text in norm_test_reviews]\n",
    "y_ts = le.fit_transform(test_sentiments)\n",
    "y_test = keras.utils.to_categorical(y_ts, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: ['negative' 'positive' 'negative'] \n",
      "Encoded Labels: [0 1 0] \n",
      "One hot encoded Labels:\n",
      " [[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print class label encoding map and encoded labels\n",
    "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_ts[:3], \n",
    "      '\\nOne hot encoded Labels:\\n', y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 27s, sys: 598 ms, total: 36min 27s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build word2vec model\n",
    "w2v_num_features = 512\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
    "                                   min_count=10, sample=1e-3, workers=16)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=w2v_num_features)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering with GloVe model\n",
    "train_nlp = [tn.nlp_vec(item) for item in norm_train_reviews]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "test_nlp = [tn.nlp_vec(item) for item in norm_test_reviews]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (35000, 512)  Test features shape: (15000, 512)\n",
      "GloVe model:> Train features shape: (35000, 300)  Test features shape: (15000, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n",
    "print('GloVe model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with deep neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Deep neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, input_shape=(num_input_features,), kernel_initializer='glorot_uniform'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(512, kernel_initializer='glorot_uniform'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(512, kernel_initializer='glorot_uniform'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(2))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample deep architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1207pt\" viewBox=\"0.00 0.00 273.00 1207.00\" width=\"273pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1203)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1203 269,-1203 269,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 3006554754296 -->\n",
       "<g class=\"node\" id=\"node1\"><title>3006554754296</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-1079.5 37.5,-1125.5 227.5,-1125.5 227.5,-1079.5 37.5,-1079.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-1098.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-1079.5 88.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-1102.5 144.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-1079.5 144.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-1110.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-1102.5 227.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-1087.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3006554754072 -->\n",
       "<g class=\"node\" id=\"node2\"><title>3006554754072</title>\n",
       "<polygon fill=\"none\" points=\"0,-996.5 0,-1042.5 265,-1042.5 265,-996.5 0,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-1015.8\">BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"126,-996.5 126,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"126,-1019.5 182,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"182,-996.5 182,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-1027.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"182,-1019.5 265,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-1004.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3006554754296&#45;&gt;3006554754072 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>3006554754296-&gt;3006554754072</title>\n",
       "<path d=\"M132.5,-1079.37C132.5,-1071.15 132.5,-1061.66 132.5,-1052.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-1052.61 132.5,-1042.61 129,-1052.61 136,-1052.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3006554754128 -->\n",
       "<g class=\"node\" id=\"node3\"><title>3006554754128</title>\n",
       "<polygon fill=\"none\" points=\"26,-913.5 26,-959.5 239,-959.5 239,-913.5 26,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-932.8\">Activation</text>\n",
       "<polyline fill=\"none\" points=\"100,-913.5 100,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"100,-936.5 156,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"156,-913.5 156,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-944.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"156,-936.5 239,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-921.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3006554754072&#45;&gt;3006554754128 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>3006554754072-&gt;3006554754128</title>\n",
       "<path d=\"M132.5,-996.366C132.5,-988.152 132.5,-978.658 132.5,-969.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-969.607 132.5,-959.607 129,-969.607 136,-969.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3006554751832 -->\n",
       "<g class=\"node\" id=\"node4\"><title>3006554751832</title>\n",
       "<polygon fill=\"none\" points=\"31,-830.5 31,-876.5 234,-876.5 234,-830.5 31,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-849.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"95,-830.5 95,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"95,-853.5 151,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"151,-830.5 151,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-861.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"151,-853.5 234,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-838.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3006554754128&#45;&gt;3006554751832 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>3006554754128-&gt;3006554751832</title>\n",
       "<path d=\"M132.5,-913.366C132.5,-905.152 132.5,-895.658 132.5,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-886.607 132.5,-876.607 129,-886.607 136,-886.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008311609272 -->\n",
       "<g class=\"node\" id=\"node5\"><title>3008311609272</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-747.5 37.5,-793.5 227.5,-793.5 227.5,-747.5 37.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-766.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-747.5 88.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-770.5 144.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-747.5 144.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-778.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-770.5 227.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-755.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3006554751832&#45;&gt;3008311609272 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>3006554751832-&gt;3008311609272</title>\n",
       "<path d=\"M132.5,-830.366C132.5,-822.152 132.5,-812.658 132.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-803.607 132.5,-793.607 129,-803.607 136,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008312129464 -->\n",
       "<g class=\"node\" id=\"node6\"><title>3008312129464</title>\n",
       "<polygon fill=\"none\" points=\"0,-664.5 0,-710.5 265,-710.5 265,-664.5 0,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-683.8\">BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"126,-664.5 126,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"126,-687.5 182,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"182,-664.5 182,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-695.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"182,-687.5 265,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-672.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3008311609272&#45;&gt;3008312129464 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>3008311609272-&gt;3008312129464</title>\n",
       "<path d=\"M132.5,-747.366C132.5,-739.152 132.5,-729.658 132.5,-720.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-720.607 132.5,-710.607 129,-720.607 136,-720.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008312505736 -->\n",
       "<g class=\"node\" id=\"node7\"><title>3008312505736</title>\n",
       "<polygon fill=\"none\" points=\"26,-581.5 26,-627.5 239,-627.5 239,-581.5 26,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-600.8\">Activation</text>\n",
       "<polyline fill=\"none\" points=\"100,-581.5 100,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"100,-604.5 156,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"156,-581.5 156,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-612.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"156,-604.5 239,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-589.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3008312129464&#45;&gt;3008312505736 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>3008312129464-&gt;3008312505736</title>\n",
       "<path d=\"M132.5,-664.366C132.5,-656.152 132.5,-646.658 132.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-637.607 132.5,-627.607 129,-637.607 136,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008313246272 -->\n",
       "<g class=\"node\" id=\"node8\"><title>3008313246272</title>\n",
       "<polygon fill=\"none\" points=\"31,-498.5 31,-544.5 234,-544.5 234,-498.5 31,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-517.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"95,-498.5 95,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"95,-521.5 151,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"151,-498.5 151,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-529.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"151,-521.5 234,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-506.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3008312505736&#45;&gt;3008313246272 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>3008312505736-&gt;3008313246272</title>\n",
       "<path d=\"M132.5,-581.366C132.5,-573.152 132.5,-563.658 132.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-554.607 132.5,-544.607 129,-554.607 136,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008313030920 -->\n",
       "<g class=\"node\" id=\"node9\"><title>3008313030920</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-415.5 37.5,-461.5 227.5,-461.5 227.5,-415.5 37.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-434.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-415.5 88.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-438.5 144.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-415.5 144.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-446.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-438.5 227.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-423.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3008313246272&#45;&gt;3008313030920 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>3008313246272-&gt;3008313030920</title>\n",
       "<path d=\"M132.5,-498.366C132.5,-490.152 132.5,-480.658 132.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-471.607 132.5,-461.607 129,-471.607 136,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008313385704 -->\n",
       "<g class=\"node\" id=\"node10\"><title>3008313385704</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 265,-378.5 265,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-351.8\">BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"126,-332.5 126,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"126,-355.5 182,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"182,-332.5 182,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-363.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"182,-355.5 265,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-340.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3008313030920&#45;&gt;3008313385704 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>3008313030920-&gt;3008313385704</title>\n",
       "<path d=\"M132.5,-415.366C132.5,-407.152 132.5,-397.658 132.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-388.607 132.5,-378.607 129,-388.607 136,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008313387272 -->\n",
       "<g class=\"node\" id=\"node11\"><title>3008313387272</title>\n",
       "<polygon fill=\"none\" points=\"26,-249.5 26,-295.5 239,-295.5 239,-249.5 26,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-268.8\">Activation</text>\n",
       "<polyline fill=\"none\" points=\"100,-249.5 100,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"100,-272.5 156,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"156,-249.5 156,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-280.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"156,-272.5 239,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-257.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3008313385704&#45;&gt;3008313387272 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>3008313385704-&gt;3008313387272</title>\n",
       "<path d=\"M132.5,-332.366C132.5,-324.152 132.5,-314.658 132.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-305.607 132.5,-295.607 129,-305.607 136,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008314224144 -->\n",
       "<g class=\"node\" id=\"node12\"><title>3008314224144</title>\n",
       "<polygon fill=\"none\" points=\"31,-166.5 31,-212.5 234,-212.5 234,-166.5 31,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-185.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"95,-166.5 95,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"95,-189.5 151,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"151,-166.5 151,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-197.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"151,-189.5 234,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 3008313387272&#45;&gt;3008314224144 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>3008313387272-&gt;3008314224144</title>\n",
       "<path d=\"M132.5,-249.366C132.5,-241.152 132.5,-231.658 132.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-222.607 132.5,-212.607 129,-222.607 136,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008314222128 -->\n",
       "<g class=\"node\" id=\"node13\"><title>3008314222128</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-83.5 37.5,-129.5 227.5,-129.5 227.5,-83.5 37.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63\" y=\"-102.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-83.5 88.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"88.5,-106.5 144.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-83.5 144.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"144.5,-106.5 227.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-91.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 3008314224144&#45;&gt;3008314222128 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>3008314224144-&gt;3008314222128</title>\n",
       "<path d=\"M132.5,-166.366C132.5,-158.152 132.5,-148.658 132.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-139.607 132.5,-129.607 129,-139.607 136,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3008314655968 -->\n",
       "<g class=\"node\" id=\"node14\"><title>3008314655968</title>\n",
       "<polygon fill=\"none\" points=\"32.5,-0.5 32.5,-46.5 232.5,-46.5 232.5,-0.5 32.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69.5\" y=\"-19.8\">Activation</text>\n",
       "<polyline fill=\"none\" points=\"106.5,-0.5 106.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"106.5,-23.5 162.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"162.5,-0.5 162.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-31.3\">(None, 2)</text>\n",
       "<polyline fill=\"none\" points=\"162.5,-23.5 232.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 3008314222128&#45;&gt;3008314655968 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>3008314222128-&gt;3008314655968</title>\n",
       "<path d=\"M132.5,-83.3664C132.5,-75.1516 132.5,-65.6579 132.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-56.6068 132.5,-46.6068 129,-56.6069 136,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3006554753568 -->\n",
       "<g class=\"node\" id=\"node15\"><title>3006554753568</title>\n",
       "<polygon fill=\"none\" points=\"80.5,-1162.5 80.5,-1198.5 184.5,-1198.5 184.5,-1162.5 80.5,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.5\" y=\"-1176.8\">3006554753568</text>\n",
       "</g>\n",
       "<!-- 3006554753568&#45;&gt;3006554754296 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>3006554753568-&gt;3006554754296</title>\n",
       "<path d=\"M132.5,-1162.25C132.5,-1154.36 132.5,-1144.75 132.5,-1135.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136,-1135.59 132.5,-1125.59 129,-1135.59 136,-1135.59\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 3500 samples\n",
      "Epoch 1/10\n",
      "31500/31500 [==============================] - 8s 250us/step - loss: 0.3378 - acc: 0.8598 - val_loss: 0.3114 - val_acc: 0.8714\n",
      "Epoch 2/10\n",
      "31500/31500 [==============================] - 6s 204us/step - loss: 0.2877 - acc: 0.8808 - val_loss: 0.2968 - val_acc: 0.8806\n",
      "Epoch 3/10\n",
      "31500/31500 [==============================] - 6s 204us/step - loss: 0.2766 - acc: 0.8854 - val_loss: 0.3043 - val_acc: 0.8726\n",
      "Epoch 4/10\n",
      "31500/31500 [==============================] - 6s 203us/step - loss: 0.2702 - acc: 0.8888 - val_loss: 0.2964 - val_acc: 0.8786\n",
      "Epoch 5/10\n",
      "31500/31500 [==============================] - 6s 204us/step - loss: 0.2656 - acc: 0.8896 - val_loss: 0.3058 - val_acc: 0.8794\n",
      "Epoch 6/10\n",
      "31500/31500 [==============================] - 6s 204us/step - loss: 0.2618 - acc: 0.8916 - val_loss: 0.3128 - val_acc: 0.8694\n",
      "Epoch 7/10\n",
      "31500/31500 [==============================] - 6s 203us/step - loss: 0.2562 - acc: 0.8924 - val_loss: 0.3009 - val_acc: 0.8746\n",
      "Epoch 8/10\n",
      "31500/31500 [==============================] - 6s 204us/step - loss: 0.2495 - acc: 0.8954 - val_loss: 0.3128 - val_acc: 0.8766\n",
      "Epoch 9/10\n",
      "31500/31500 [==============================] - 6s 203us/step - loss: 0.2456 - acc: 0.8964 - val_loss: 0.3180 - val_acc: 0.8680\n",
      "Epoch 10/10\n",
      "31500/31500 [==============================] - 6s 202us/step - loss: 0.2385 - acc: 0.9014 - val_loss: 0.3126 - val_acc: 0.8717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2fd61e0f0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
    "predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8817\n",
      "Precision: 0.8818\n",
      "Recall: 0.8817\n",
      "F1 Score: 0.8817\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.87      0.89      0.88      7510\n",
      "   negative       0.89      0.87      0.88      7490\n",
      "\n",
      "avg / total       0.88      0.88      0.88     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6704      806\n",
      "        negative        969     6521\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                      classes=['positive', 'negative'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dnn = construct_deepnn_architecture(num_input_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 3500 samples\n",
      "Epoch 1/10\n",
      "31500/31500 [==============================] - 8s 246us/step - loss: 0.4116 - acc: 0.8204 - val_loss: 0.4466 - val_acc: 0.8066\n",
      "Epoch 2/10\n",
      "31500/31500 [==============================] - 6s 194us/step - loss: 0.3348 - acc: 0.8547 - val_loss: 0.3553 - val_acc: 0.8449\n",
      "Epoch 3/10\n",
      "31500/31500 [==============================] - 6s 193us/step - loss: 0.3134 - acc: 0.8643 - val_loss: 0.3433 - val_acc: 0.8489\n",
      "Epoch 4/10\n",
      "31500/31500 [==============================] - 6s 195us/step - loss: 0.2973 - acc: 0.8714 - val_loss: 0.3539 - val_acc: 0.8474\n",
      "Epoch 5/10\n",
      "31500/31500 [==============================] - 6s 195us/step - loss: 0.2863 - acc: 0.8791 - val_loss: 0.3429 - val_acc: 0.8480\n",
      "Epoch 6/10\n",
      "31500/31500 [==============================] - 6s 195us/step - loss: 0.2746 - acc: 0.8850 - val_loss: 0.3737 - val_acc: 0.8477\n",
      "Epoch 7/10\n",
      "31500/31500 [==============================] - 6s 196us/step - loss: 0.2611 - acc: 0.8889 - val_loss: 0.3854 - val_acc: 0.8329\n",
      "Epoch 8/10\n",
      "31500/31500 [==============================] - 6s 192us/step - loss: 0.2462 - acc: 0.8971 - val_loss: 0.3621 - val_acc: 0.8554\n",
      "Epoch 9/10\n",
      "31500/31500 [==============================] - 6s 195us/step - loss: 0.2338 - acc: 0.9033 - val_loss: 0.4550 - val_acc: 0.8269\n",
      "Epoch 10/10\n",
      "31500/31500 [==============================] - 6s 193us/step - loss: 0.2214 - acc: 0.9082 - val_loss: 0.4053 - val_acc: 0.8529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2fc1b60b8>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "glove_dnn.fit(train_glove_features, y_train, epochs=10, batch_size=batch_size, \n",
    "              shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred = glove_dnn.predict_classes(test_glove_features)\n",
    "predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8596\n",
      "Precision: 0.8596\n",
      "Recall: 0.8596\n",
      "F1 Score: 0.8596\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.86      0.86      0.86      7510\n",
      "   negative       0.86      0.85      0.86      7490\n",
      "\n",
      "avg / total       0.86      0.86      0.86     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6496     1014\n",
      "        negative       1092     6398\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                      classes=['positive', 'negative'])  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
